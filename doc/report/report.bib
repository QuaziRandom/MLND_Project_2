@article{Metsis06,
	author = {Vangelis Metsis and Ion Androutsopoulos and Georgios Paliouras},
	title = {Spam Filtering with Naive Bayes -- Which Naive Bayes?},
	journal = {Third Conference on Email and Anti-Spam (CEAS)},
	year = {2006},
}
@article{Rish01,
	author = {I. Rish},
	title = {An empirical study of the naive Bayes classifier},
	journal = {Proceedings of IJCAI-01 workshop on Empirical Methods in AI},
	year = {2001},
	pages = {41--46},
}
@inproceedings{boser1992,
	title={A training algorithm for optimal margin classifiers},
	author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
	booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
	pages={144--152},
	year={1992},
	organization={ACM}
}
@article{joachims1998,
	title={Text categorization with support vector machines: Learning with many relevant features},
	author={Joachims, Thorsten},
	journal={European Conference on Machine Learning (ECML)},
	pages={137--142},
	year={1998},
	publisher={Springer Berlin/Heidelberg}
}
@inproceedings{joachims2005,
	title={A support vector method for multivariate performance measures},
	author={Joachims, Thorsten},
	booktitle={Proceedings of the 22nd international conference on Machine learning},
	pages={377--384},
	year={2005},
	organization={ACM}
}
@article{hsu2003,
	title={A practical guide to support vector classification},
	author={Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen and others},
	year={2003}
}
@article{shapire1990,
	author="Schapire, Robert E.",
	title="The strength of weak learnability",
	journal="Machine Learning",
	volume="5",
	number="2",
	pages="197--227",
	year="1990"
	abstract="This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.",
	issn="1573-0565",
	doi="10.1007/BF00116037",
	url="http://dx.doi.org/10.1007/BF00116037"
}
@article{Freund1997,
title = "A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting ",
journal = "Journal of Computer and System Sciences ",
volume = "55",
number = "1",
pages = "119 - 139",
year = "1997",
note = "",
issn = "0022-0000",
doi = "http://dx.doi.org/10.1006/jcss.1997.1504",
url = "http://www.sciencedirect.com/science/article/pii/S002200009791504X",
author = "Yoav Freund and Robert E Schapire"
}
@inbook{Schapire2013,
	author="Schapire, Robert E.",
	editor="Sch{\"o}lkopf, Bernhard
	and Luo, Zhiyuan
	and Vovk, Vladimir",
	chapter="Explaining AdaBoost",
	title="Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik",
	year="2013",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="37--52",
	isbn="978-3-642-41136-6",
	doi="10.1007/978-3-642-41136-6_5",
	url="http://dx.doi.org/10.1007/978-3-642-41136-6_5"
}
@inbook{Olson2005,
	author="Olson, David L.",
	editor="Shi, Yong
	and Xu, Weixuan
	and Chen, Zhengxin",
	chapter="Data Set Balancing",
	title="Data Mining and Knowledge Management: Chinese Academy of Sciences Symposium CASDMKM 2004, Beijing, China, July 12-14, 2004. Revised Papers",
	year="2005",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="71--80",
	isbn="978-3-540-30537-8",
	doi="10.1007/978-3-540-30537-8_8",
	url="http://dx.doi.org/10.1007/978-3-540-30537-8_8"
}